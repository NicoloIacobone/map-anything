DPTFeature (Feature Extraction + Fusion)
Input
- PredictionHeadLayeredInput:
    -list_features: 4 tensori BCHW da layer intermedi del ViT (hooks: [2, 5, 8, 11])
    -Shapes tipiche: (B, 768/1024, H//14, W//14) (patch size 14)

Architettura
1. Input Processing (per ogni layer):
    - Conv2d(C_in → layer_dim) (proiezione canali)
    - ConvTranspose2d per upsampling differenziato:
        - Layer 0 (hook 2): 4× upsample (ConvTranspose stride=4)
        - Layer 1 (hook 5): 2× upsample (ConvTranspose stride=2)
        - Layer 2 (hook 8): 1× (no upsample)
        - Layer 3 (hook 11): 0.5× downsample (Conv stride=2)

2.Multi-Scale Fusion (bottom-up):
    - path_4 = refinenet4(layer_3_processed)               # più profondo
    - path_3 = refinenet3(path_4, layer_2_processed)       # fonde con layer intermedio
    - path_2 = refinenet2(path_3, layer_1_processed)
    - output = refinenet1(path_2, layer_0_processed)       # fonde con layer più superficiale
Ogni refinenet è un FeatureFusionBlock:
    - ResidualConvUnit (Conv3×3 + optional BatchNorm)
    - Element-wise sum delle feature a scala diversa
    - Riproiezione con Conv2d(feature_dim → feature_dim)

3. Output:
    - features_upsampled_8x: (B, 256, 8×(H//14), 8×(W//14))

4. Componenti Chiave
    - make_scratch: crea layer di proiezione (layer_rn) per uniformare i canali
    - make_fusion_block: blocchi residuali con fusion
    - No LayerNorm: solo Conv + optional BatchNorm
    - Gradient Checkpointing: opzionale per ridurre memoria


Top-level modules:
- encoder: DINOv2Encoder (304,367,616 params)
- ray_dirs_encoder: DenseRepresentationEncoder (31,295,068 params)
- depth_encoder: DenseRepresentationEncoder (29,220,604 params)
- depth_scale_encoder: GlobalRepresentationEncoder (692,224 params)
- cam_rot_encoder: GlobalRepresentationEncoder (692,608 params)
- cam_trans_encoder: GlobalRepresentationEncoder (692,480 params)
- cam_trans_scale_encoder: GlobalRepresentationEncoder (692,224 params)
- fusion_norm_layer: LayerNorm (2,048 params)
- info_sharing: MultiViewAlternatingAttentionTransformerIFR (170,897,664 params)
- dpt_feature_head: DPTFeature (18,578,368 params)
- dpt_regressor_head: DPTRegressionProcessor (443,398 params)
- dense_head: Sequential (19,021,766 params)
- dpt_feature_head_2: DPTFeature (18,578,368 params)
- sam2_compat: SAM2CompatibilityLayer (66,304 params)
- pose_head: PoseHead (5,531,911 params)
- scale_head: MLPHead (228,145 params)
- dense_adaptor: RayDirectionsPlusDepthWithConfidenceAndMaskAdaptor (0 params)
- pose_adaptor: CamTranslationPlusQuatsAdaptor (0 params)
- scale_adaptor: ScaleAdaptor (0 params)

Quello che mi interessa è MultiViewAlternatingAttentionTransformerIFR

Found 24 transformer blocks in info_sharing.self_attention_blocks
- Block 0: 7,087,872 params
- Block 1: 7,087,872 params
...
- Block 23: 7,087,872 params